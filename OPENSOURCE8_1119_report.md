1.1	문서의 목적
1.1.1	개발 배경
오늘날 우리는 정보가 범람하고 있는 시대에 살아가고 있다. 이러한 흐름은 국가, 기업, 개인에게 큰 변화를 가져오고 있다. 수많은 정보 사이에서 우리는 가짜 정보들을 걸러내고 우리에게 최적화 된 
정보만을 골라 내야한다. 본인에게 필요한 정보를 받아들이지 못한다면 경쟁 사회에서 밀려 도태될 수밖에 없다. 도태되지 않기 위해서는 우리는 어떻게 해야 할까? 우리는 쉬우면서도 빠르게 정보에 접근하면서 사용자에게 최적화된 정보를 찾을 방법을 고안해보았다. 앞서 나온 방법으로는 우리는 인공지능과 빅데이터 기술을 생각해보았다. 인공지능과 빅데이터 기술은 굉장히 빠르게 발전되어 다양한 학습방법을 통해 발전되어가고 있다. 따라서 우리가 학습만 시켜준다면 우리의 입맛대로 설정하여 사용자들에게 쉬우면서 빠르고 정확한 정보 전달이 
가능하다고 판단하였다. 또한 구글과 네이버 등에서 제공하는 다양한 API를 통해서 데이터를 수집하여 우선적으로 데이터를 걸러낼 생각이다. 
방법은 찾았으니 이제는 최적화시킬 데이터를 선정해야 한다. 우리는 활용할 데이터의 양이 많으면서 데이터의 검색이 어려운 사진과 동영상을 선정했다. 하지만 사진은 정적인 데이터 이기 때문에 각도에 따라 데이터의 식별이 정확하지 않아 정보를 읽는 데 어려움이 있다. 우리는 어려움을 해결하고자 동영상 데이터를 활용하기로 했다. 동영상 데이터는 사진에 비해서 용량을 많이 차지하고 있어 큰 용량을 감당할 저장소와 빠른 인터넷 속도가 필요하다. 과거에 비해 현재는 빠른 인터넷 속도와 큰 용량의 데이터를 담을 수 있는 여러 서비스가 제공되고 있다. 이러한 기술의 발전을 통해 다양한 서비스들이 인기를 끌고 있다. 최근 OTT 서비스, 유튜브 등의 성장은 동영상 데이터를 꾸준히 생산하고 있다. 앞서 말했듯이 정보를 찾는데 어려우면서 부족할 일 없는 데이터로 동영상이 적합했다.
가장 진보된 기술인 인공지능, 빅데이터를 통해 동영상의 데이터를 분석하는 기술을 제공하여 다양한 사용자들이 정보를 얻는데 불편함이 없게 하는 것이 본 서비스의 목적이다.

1.1.2	가정 (기대효과/예상결과)
세계에는 유명한 연예인, 영화배우 등의 유명인이 사용하는 제품이 화제가 되고 나오면 얼마 지나지 않아 화제가 되고 동나는 현상이 나타난다. 특히 유행에 민감한 우리나라는 이러한 정보를 수집하는 것에 관심이 많다. 해당 제품에 대한 관심은 우리 서비스의 사용자를 늘릴 수 있을 것이며 관련 데이터를 효과적으로 모아서 더욱더 발전된 서비스를 제공할 수 있을 것으로 예상된다.
그 뿐만 아니라 개인이나 소매점, 작은 기업 같은 곳은 지금의 정보 홍수 시대에서 본인들에게 맞는 맞춤형 정보를 분류하고 찾아내기 쉽지 않으리라고 예상한다. 경쟁 사회에서 정보의 뒤처짐은 경쟁에서 뒤처진다는 것을 의미한다. 따라서 정보의 선점이 하나의 경쟁력이 되었고 우리 서비스는 정보를 선점하기에 용이하다. 서비스 사용자가 늘어날수록 챗봇의 성능은 향상되고 인기 있는 데이터들을 빠르게 제공함으로써 서비스의 발전을 이끌 것이다.우리 서비스는 쉬운 접근, 효과적인 성능을 통해 소비자들에게 질 좋은 정보를 제공한다. 또한, 사용자들이 직접 사용한 데이터를 수집하여 데이터의 정확도와 접근속도를 높일 것이다.
1.2	유사서비스 및 차별화
우리가 설계한 서비스와 비슷한 서비스에는 여러 가지가 있지만 그 중에서도 많이 쓰이고 있는 네이버 스마트렌즈와 비교해 보고자 한다.
 네이버의 스마트렌즈는 딥러닝 기반의 이미지 분석기술로, 키워드로 검색하는 것이 아닌 이름이나 키워드를 몰라도 관련 정보를 찾을 수 있는 이미지 검색 서비스이다. 여기서 딥러닝이란 컴퓨터가 데이터를 분석하고 스스로 학습하는 과정을 거쳐 패턴을 인식하는 능력을 갖추어 사물이나 데이터를 군집화하거나 분류하고, 컴퓨터가 사람처럼 생각하고 배울 수 있도록 하는 기술이다. 이 서비스는 사용자가 검색하는 이미지와 관련해 동일한 정보부터 유사한 정보가 제공되며 특화된 세부 정보까지 보여준다. 또한 스마트렌즈는 '검색어 추가' 기능을 통해 이미지 촬영 후 텍스트를 추가 입력해 원하는 정보의 범위를 구체화할 수 있다. 이는 우리가 설계한 제품과 비슷하다고 볼 수 있다. 예를 들면, 특정 신발 이미지를 검색한 후 텍스트로 색상, 디자인, 소재 등 검색어/키워드를 더해 검색하고자 하는 정보의 범위를 좁힐 수 있다. 또한, 제품을 촬영하게 되면 촬영한 제품 이미지와 유사한 이미지를 검색해 준다. 단순히 비슷한 이미지만 찾아주는 것이 아닌 쇼핑 정보도 함께 보여주기 때문에 가격 비교에도 용이하다.
하지만 이러한 기술에도 단점은 있다. 위에서 설명한 스마트렌즈의 경우, 촬영한 이미지에 대해 찾고자 하는 정보가 관련성과 정확성 면에서 아직 미흡하고 오답이 발생하게 된다. 그 뿐만 아니라 찾고자 하는 제품의 데이터가 적거나 애매하게 생긴 제품은 인식이 잘되지 않는다. 
예를 들자면 약통을 촬영하여 검색했을 때 정확한 정보를 얻기 힘들다고 한다. 약통은 대부분 모양이 동일하고 애매하기 때문에 우리가 찾고자 하는 정보가 나오지 않을 수 있다. 하지만 여기서 우리가 설계한 서비스와의 차별성이 드러난다. 우리는 한 면의 사진만으로 검색하는 것이 아닌, 동영상을 촬영하여 검색하기 때문에 찾고자 하는 제품의 전체적인 모양과 작은 특징들도 영상에 담겨 검색할 수 있다. 그렇기 때문에 이미지만으로 검색하는 것보다 더 정확한 정보를 얻을 수 있을 것이다.

1.3	서비스 설명
다들 한 번쯤은 원하는 상품의 세부적인 정보를 찾기 위해 여러 부분을 반복해서 탐색하며 시간을 낭비했던 경험이 있을 것이다. 정보 전달 챗봇은 이러한 사용자의 정보 검색을 보조하는 방법을 제안한다. 정보 전달 챗봇은 5~10초 사이의 영상을 입력받아 그에 맞는 키워드를 생성해 사용자에게 제안하고, 사용자에게 키워드를 선택받거나 직접 입력을 받아 정보를 제공하는 서비스이다.
예를 들어 사용자가 길거리를 지나가다 마음에 드는 스타일의 옷을 발견했을 때 사용자는 영상을 찍어 옷의 질감과 형태를 챗봇에게 전달한다. 챗봇은 전달받은 영상을 통해 영상 속 제품이 무엇인지, 혹은 상품의 모델명이 찍혔을 경우 이미지의 문자를 디지털화하는 등의 분석 과정을 통해 옷과 같은 키워드를 생성한다. 이후에 생성된 키워드들과 연관된 키워드를 사용자에게 제안하고 선택하도록 한다. 사용자가 선택한 키워드가 상품일 경우 해당 상품을 사는 쇼핑몰 사이트로 연결시켜주고, 쇼핑몰을 원하는 게 아닌 정보 수집이 목표인 경우 챗봇의 검색 모듈을 통해 정보를 사용자에게 제공한다. 만약, 사용자가 전달 받은 키워드 중 원하는 키워드가 없을 경우 추가 검색 기능을 제공해 사용자에게 추가적으로 입력을 받아 그에 맞는 정보를 전달한다. 이처럼 사용자는 정보 전달 챗봇을 통해 마음에 드는 상품의 모델명을 힘들게 찾아보고, 검색창에 일일이 입력해 검색해야 하는 불편함을 덜어낼 수 있다.

. 설계개요
2.1 필수 환경
 개발 환경 및 도구

![스크린샷 2022-12-03 23 02 22](https://user-images.githubusercontent.com/98390300/205444655-38f87243-14ca-4136-a80e-88af78ab6f8a.png)

2.2 시스템 예상 설계도
 
본 팀이 고안한 AI정보전달 챗봇의 주요 기능은 다음과 같다.

![스크린샷 2022-12-03 23 04 15](https://user-images.githubusercontent.com/98390300/205444720-019b6ca8-14c8-4ed0-9f74-5b34fefa8fa7.png)

 2.1.1 챗봇 구현
 사용자가 원하는 영상을 챗봇 APP에 업로드 하면 챗봇 데이터 생성 모듈을 통해 영상에서 추출한 음성 파일과 프레임 파일을 생성한다. 음성 파일은 STT에 의해 텍스트로 추출되고, khaiii와 형태소 분석 및 BERT(koBERT)에 의해 핵심 문장, 단어를 생성한다. 또한 프레임은 OpenCV에 의해 이전 프레임과 차이값( |현재 프레임 – 이전 프레임|)이 기준값 보다 큰 프레임만을 저장하여 키워드 생성 모듈로 넘긴다.

2.1.2 키워드 리스트 생성 모듈
챗봇에서 받아온 프레임을 jpg, png 파일로 변환시킨 뒤, Google Cloud API와 Tesseract OCR을 통해 이미지에서 키워드를 추출한다.
먼저, 이미지 파일 Google Cloud API를 활용하여 키워드를 NAVER 연관검색어 API에 전달한다. 또한 Google Cloud API의 반환 값이 정확도가 낮을 것의 경우를 대비하여 Tesseract OCR로 이미지에 있는 텍스트를 모두 추출한다. 그리고 추출한 텍스트를 keyBERT를 이용하여 키워드를 추출하고 이 키워드를 NAVER 연관검색어 API에 전달한다.
키워드를 받아온 NAVER 연관검색어 API를 통해 유사어나, 관련된 단어들을 생성한다. 이후 키워드를 포함하여 유사어 리스트들을 챗봇에 전달한다. 사용자는 이 리스트들을 보고 자신의 의도에 가장 적합한 리스트를 선택한다.

2.1.3 검색 모듈
사용자가 선택한 리스트에 따라 사용하는 오픈소스가 다르다.
사용자가 선택한 항목이 쇼핑몰이면 NAVER 쇼핑 API를 사용한다. 해당 물품과 유사한 쇼핑몰을 검색하여 상위 5개를 사용자에게 쇼핑몰을 전달한다.
그외의 항목은 elasticsearch를 사용한다. 사용자가 문장을 받으면 답변에 해당하는 문장을 사용자에게 전달한다. 이때 정확한 값을 추출하기 위해 khaiii와 keyBERT를 사용하여 단어검색도 가능하도록 한다.

3.1 사용된 오픈소스
 khaiii는 “Kakao Hangul Analyzer III”의 줄임말로 카카오에서 개발한 세 번째 형태소 분석기로 세종 코퍼스를 이용하여 CNN(Convolutional Neural Network, 합성곱 신경망) 기술을 적용해 학습한다. 디코더를 C++로 구현하여 GPU 없이도 비교적 빠르게 동작하며, Python 바인딩을 제공하고 있어서 편리하게 사용할 수 있다.
규칙 기반으로 동작하여 사람이 직접 규칙을 입력해야 했던 기존의 분석기인 dha1, dha2 와는 다르게 khaiii는 데이터 기반으로 동작하기 때문에 기계학습 알고리즘(딥러닝)을 사용한다. 기계학습을 위해서 신경망 알고리즘들 중에서 CNN을 사용했다. 입력하는 경우 각각의 음절이 분류 대상이 된다.
형태소 분석 결과를 다시 형태소 각각의 음절 별로 나누어 IOB1 방식으로 나타내면 다음 사진과 같고, 나누어진 각각의 음절들은 다음 사진과 같이 정렬된다.

![image](https://user-images.githubusercontent.com/98390300/205445042-b8e15323-3079-4f71-8cbb-d1f61ff88f98.png)

![image](https://user-images.githubusercontent.com/98390300/205445036-d1d63b2b-48fb-4983-b81e-a2181fa783c3.png)



khaiii의 성능에 대해 얘기해 보면, 정확도에서는 파라미터 win의 경우, 3 또는 4에서 가장 좋은 성능을 보였고 그 이상에서는 성능이 떨어졌다. 파라미터 emb의 경우, 150까지는 성능도 같이 높아지다가 그 이상에서는 별 차이가 없다. 다음 사진은 정확도에 관한 사진이다.
 
![image](https://user-images.githubusercontent.com/98390300/205445027-76a8f85d-6aa7-4d7e-a320-293c308a9523.png)

(*win : CNN 모델의 주요 하이퍼 파라미터가 분류하려는 음절의 좌/우 문맥의 크기를 나타내는 값, win의 값은 {2, 3, 4, 5, 7, 10}의 값을 가짐)
(*emb : 음절 임베딩의 차원을 나타내는 값, emb의 값은 {20, 30, 40, 50, 70, 100, 150, 200, 300, 500}의 값을 가짐) 
(*성능 지표는 정확률과 재현율의 조화 평균값인 F-Score이다.)

KoBERT는 Korean BERT (Bidirectional Encoder Representations from Transformers)의 줄임말이며 BERT의 한국어 성능 한계를 극복하기 위해 개발되었다. 따라서 koBERT는 BERT와 구조가 동일하므로 BERT를 설명하고자 한다.
BERT란 Google에서 개발한 자연어 처리 사전 교육을 위한 변압기 기반 기계 학습 기술이다. BERT의 특징으로는 두 가지가 있다.
 첫 번째로는 사전 학습된 대용량의 레이블링되지 않는 데이터를 이용하여 언어 모델을 학습하고 이를 토대로 문서 분류, 질의응답, 번역 등을 위한 신경망을 추가하여 전이 학습을 하고 두 번째로는 대량의 단어 임베딩에 대해 사전 학습이 되어있는 모델을 제공하기 때문에 비교적 적은 자원으로 자연어 처리 일이 가능하다는 특징이 있다.
 BERT는 사전 학습을 위해 Fine-tuning으로 접근한다. 여기서 Fine-tuning 이란 어떤 특정 작업에 대한 매개변수를 최소화하여 범용적인 사전 학습된 모델을 사용한다. 그리고 특정 작업을 수행할 경우 그에 맞게 Fine-tuning을 적용한다.
 다음 그림은 BERT를 이용한 자연어 처리 과정이며 Pre-training 과정과 이를 Fine-tuning 하여 여러 자연어 처리를 수행하는 과정이다.
 
![image](https://user-images.githubusercontent.com/98390300/205445022-3f166bb4-2bdb-4cce-a00c-212aba807177.png)

 
 BERT의 Pre-training 과정에서 모든 레이어에서 Masked language Model, Next Sentence Prediction 두 가지 방법으로 BERT를 양방향 학습시켜 문맥을 파악 시킨다.
 Masked language Model은 input 토큰의 일부분을 랜덤하게 마스킹 하는 기법이고 다음 사진과 같이 진행된다. 
 
![image](https://user-images.githubusercontent.com/98390300/205445011-2e796bd4-74ed-46ae-b108-5ef9591fbc26.png)

 Mask 토큰의 마지막 Hidden vector 값이 활성화 함수인 Softmax 함수로 들어가 특정 단어를 출력하는 방식이다. Mask 토큰은 15%의 비율로 랜덤하게 생성된다. masking 비율의 80%는 mask 토큰으로 변환되며 10%는 랜덤 한 단어, 나머지 10%는 기존 단어를 사용하여 문맥 정보를 활용할 수 있는 모델이 된다.
 다음은 Next Sentence Prediction이다. BERT는 두 문장의 관계를 파악하기 위해 원래 연결되어 있던 문장인지 사전 학습하는 것이다. 학습의 방법은 50%는 연결된 문장, 50%는 랜덤하게 뽑힌 문장을 집어넣는다. SEP이라는 특수 토큰을 활용해 문장을 분리하고 BERT는 임의의 문장이 첫 번째 문장에서 분리된다는 가정 하에 두 번째 문장이 임의의 문장인지에 대한 여부를 예측한다.
 BERT는 이미 Pre-trained 되어 있지만 아무래도 한국어에서의 자연어 처리 부분에서 성능을 발휘하기 힘들기에 위키피디아, 뉴스 등에서 수집한 수백만 개의 한국어 문장으로 이루어진 대규모 말뭉치를 학습시켰다. 한국어의 불규칙한 언어 변화 특성을 극복하기 위해 데이터 기반 토큰화 기법을 적용하여 기존보다 성능 향상을 이끌어 냈다. 이를 통해 기존 BERT의 한국어 성능 한계를 극복했다.
 BERT as a service는 BERT와 연동되는 구조이다. BERT 모델을 Serving 하기 위해 사용한다. 학습된 BERT에서 문장을 정해진 길이의 Vector 값으로 수치화한다. Vector의 각 요소는 문장의 의미를 인코딩하면 BERT as a service는 짧은 코드를 통해 서비스로 제공할 수 있다. 이것의 장점으로는 빠른 속도, 짧은 코드로 인한 쉬운 접근, 다양한 환경 제공이다. 
 BERT as a service에서 BERT를 효율적으로 서빙하기 위해서 다음 소개할 풀링 전략을 사용한다. 보통 마지막 레이어는 이미 학습된 레이어이기 때문에 MLM, NSP에 의해 편향되어 있다. 그렇기에 편향되지 않은 레이어에 풀링을 적용한다. 
 
![image](https://user-images.githubusercontent.com/98390300/205445004-a8a4b618-2179-45ab-bef1-f981613bcf4c.png)

위에 사진은 PCA를 활용하여 2차원으로 나타낸 BERT as a service로 나타낸 BERT 레이어들이다. 편향된 레이어와 그렇지 않은 레이어를 확인하기 위해 UCI-news Aggregator Dataset과 pretrained uncased_L-12_H-768_A-12를 이용해 시각화한 모델이다. 무작위 기사 제목을 뽑아 서로 다른 레이더에서 인코딩하고 2차원으로 나타냈고 데이터엔 각각의 4가지 데이터를 다른 색깔로 표현해 보았다.
비슷한 층의 레이어는 비슷한 표현을 뽑아냈다고 생각하면 된다. 1번화 6번 레이어의 차이가 극심해 보이는데 이는 서로 다른 의미를 표현하고 있다고 판단된다.
6번 레이어 같은 경우 원래 의미 자체를 보존하기 때문에 첫 레이어와 마지막 레이어를 교환하고 비교하며 차이를 알아볼 수 있다.
이러한 BERT as a service는 짧고 빠르게 문장을 정해진 길이의 vector로 표현하기 위해서 BERT를 사용해야 하고 이 메시지를 전달해 줄 고성능 비동기 메시징 라이브러리로 ZeroMQ를 사용하여 서비스를 제공한다.

OpenCV
 opencv란 Open Source Computer Vision의 약자이며 오픈 소스 컴퓨터 비전 라이브러리 중 하나로 크로스 플랫폼과 실시간 이미지 프로세싱에 중점을 둔 라이브러리이다. 즉, 객체, 얼굴, 행동 인식, 모션 추적 등의 응용 프로그램에서 사용하는 영상처리에 관한 라이브러리이다. 또한 내부적으로 TensorFlow, Torch 같은 딥러닝 프레임워크를 지원한다.
BSD 라이선스를 사용하므로 학술적 및 상업적으로도 이용 가능하다. OpenCV에서는 Haar-Cascade 트레이너와 검출기를 모두 제공한다. 특정 객체를 트레이닝시켜 학습 데이터로 저장하고, 학습 데이터를 이용하여 특정 객체를 검출한다. (*Haar Cascade Classifier는 특정 형태의 물체를 찾고자 할 때 사용할 수 있는 방법 -> 여러 객체 이미지와 객체가 아닌 이미지를 cascade 함수로 트레이닝 시켜 객체 검출을 하는 머신러닝 기반 접근 방법이다.)
*OpenCV의 영상 처리 과정은 다음과 같다. 
영상 입출력 -> 전처리 -> 특징 추출 -> 객체 검출, 영상 분할 -> 분석(객체 인식, 움직임 분석 등) -> 화면 출력, 최종 판단
==> OpenCV는 사물을 인식할 때 특징점(keypoint)을 추출하여 분석한다. 영상에서 가장 좋은 특징점은 코너점이다. 특징점을 검출기는 Harris Corner검출, Shi&Tomasi검출, GFTT검출 등이 있다.
Harris Corner 검출을 위해서는 아래의 과정이 필요하다.
1. 영상의 특정 픽셀 위치와 그 주변점들과의 x방향, y방향의 밝기 차이를 계산한다.

![image](https://user-images.githubusercontent.com/98390300/205444983-3dbcba46-a715-40fc-83fe-f8bcaefe9085.png)

2. 계산한 값이 x방향, y방향으로 모두 값이 크게 나타나는지를 계산하여 코너 여부를 판단한다.
 
![image](https://user-images.githubusercontent.com/98390300/205444974-37935d96-1d82-4c97-a4a2-65396d01a7c6.png)


Harris Corner 검출은 소벨 미분으로 경계값을 검출하면서 경계값의 경사도 변화량을 측정하여 변화량이 수직, 수평, 대각선 방향으로 크게 변화하는 것을 코너로 판단한다.

![image](https://user-images.githubusercontent.com/98390300/205444969-0f6bb543-1d01-447c-8213-3fa638b330dd.png)

 
Harris Corner 검출기로 코너를 검출한 영상의 예시는 다음 사진과 같다.

![image](https://user-images.githubusercontent.com/98390300/205444937-52f62d08-8f34-41ef-a060-7a5539089f1b.png)

keyBERT
keyBERT는 앞서 말한 BERT를 적용한 오픈 소스 파이썬 모듈이다. keyBERT는 원본 문서를 가장 잘 나타내는 중요한 용어 또는 구문을 찾아내는 키워드를 추출하는 작업을 한다.
keyBERT의 원리는 BERT를 이용해 문서 레벨 (document-level)에서의 주제 (representation)를 파악하도록 하고, N-gram을 위해 단어(자연어)를 기계가 이해할 수 있는 숫자의 나열인 벡터로 바꾸는 임베딩 작업을 한다. 여기서 N-gram 이란 Bag of Words, TF-IDF(Term Frequency - Inverse Document Frequency) 와 같이 사용되는 횟수 기반의 벡터 표현 방식을 사용하는 언어 모델이다. 이후 N-gram 모델을 사용해 문서의 각 문장을 단어로 나누어 준다. 코사인 유사도를 계산하여 어떤 N-gram 단어 또는 구가 문서와 가장 유사한지 찾아낸다. 그리고 가장 유사한 단어들은 문서를 가장 잘 설명할 수 있는 키워드로 분류된다.

![image](https://user-images.githubusercontent.com/98390300/205444927-2a04df25-5d75-4c6c-9543-dd1754a7e02e.png)

 
또한, keyBERT는 중복된 의미의 키워드가 너무 많이 추출되는 경우에 대비해 결과 키워드에 다양성을 도입하는 MSS, MMR 두 가지 방법을 포함한다. MSS는 candidate-document 간 거리는 최소로 하면서 candidate-candidate 간 거리는 최대로 함으로써 의미적으로 풍부한 키워드 set을 얻는다. 문서와 가장 유사한 words/phrases 2개를 사용해 이 두 개의 top_n 단어에서 모든 top_n combination을 취해 코사인 유사도를 구한다. 여기서 유사도가 가장 낮은 조합을 추출하는 방법론이다. MMR은 텍스트 요약 작업에서 중복성을 최소화하고 결과의 다양성을 극대화하기 위해 노력한다. MRR은 문서와 가장 유사한 키워드를 선택하는 것으로 시작한다. 그런 다음 문서와 비슷하면서도 이미 선택한 키워드와 비슷하지 않은 새 후보를 반복s적으로 선택한다. 
이처럼 keyBERT는 비교적 단순함에도 불구하고 효율적이기 때문에 키워드 추출을 수행하는 경우 유용하게 사용된다.






Tesseract-OCR
Tesseract는 1984~1994년에 HP 연구소에서 개발되었고, 2005년 오픈소스로 개방되었다. 이미지로부터 텍스트를 인식하고, 추출하는 소프트웨어를 일반적으로 OCR(Optical Character Recognation)이라고 한다. Tesseract-OCR은 현재까지도 LSTM과 같은 딥러닝 방식을 통해 텍스트 인식률을 지속적으로 개선하고 있다.
Tesseract-OCR의 동작 과정은 첫째, 이미지를 입력받아 임계를 처리한다. 여기서 임계 처리란 임계값을 기준으로 이미지를 이진화 하는 것을 말한다. 이진화를 했을 때 0과 255로 이루어진 흑백 이미지로 만들 수 있고 값이 2개만 있기 때문에 True, False 형태로 바꾸어서 다른 작업을 수행하는 것이 가능하다. 둘째, 이진화 한 이미지의 연결된 구성요소를 분석하여 각 구성요소의 외곽선을 추출하고, 이를 Blob 단위로 저장한다. 셋째, 텍스트 라인(text line)을 분석하여 구성요소를 체계화한 뒤, 문자의 자간에 따라 단어 단위로 나눈다. 마지막으로, 단어 단위로 나뉜 요소를 단어 단위와 페이지 단위로 인식한 후 텍스트 파일을 생성한다. 

![image](https://user-images.githubusercontent.com/98390300/205444920-168aee00-3bf3-4359-859b-de2e68c24f78.png)


Tesseract는 조명, 각도, 폰트 모양에 따라 인식률이 저하될 수 있다. 이에 Tesseract-OCR의 인식률을 개선하는 방법으로는 세 가지가 있다. 첫 번째는 언어 데이터를 학습시키는 것이다.  언어 셋을 통해 언어 데이터를 학습해야 하는데, 특히 한글은 데이터 셋 자체가 매우 적어 많은 학습이 필요하다. 두 번째는 올바른 규격을 사용하는 것이다. Tesseract-OCR에서는 12pt 정도의 크기, 반듯한 글자, 글자 테두리 처리, 문장 분할 등을 권장하고 있다. 세 번째는 이미지를 전처리 하는 것이다. 이는 이미지 처리 시에 사용되는 알고리즘을 좀 더 효율적으로 활용하기 위해 유의미한 정보로 가공하는 과정이다. Gray Scale 적용과 Thresh Hold 기법을 이용해 특징 추출, 글자가 필기체인 경우 이를 정자체로 매핑, 3차원 이미지를 2차원으로 변환시켜주는 Perspective Transform 등 이미지를 전처리하여 Tesseract-OCR의 인식률을 높일 수 있다.


Elasticsearch
Elasticsearch는 Apache Lucene(아파치 루씬) 기반의 분산 검색 엔진이다. Elasticsearch를 통해 Java에서 개발한 정보 검색용 루씬 라이브러리를 단독으로 사용할 수 있으며, 방대한 양의 데이터를 신속하게 저장, 검색, 분석을 수행할 수 있다. Elasticsearch는 아파치 라이선스 조항에 의거하여 오픈소스로 출시되어 있다. 공식 클라이언트들은 자바(Java), C#닷넷(.NET), PHP, 파이썬(Python), 그루비(Groovy) 등 수많은 언어로 이용이 가능하다. 
Elasticsearch는 여러 문서에서 특정 문자열을 검색하는 분산처리를 통해 실시간성으로 빠른 검색이 가능하다. 특특히 기존의 데이터로 처리하기 힘든 대량의 텍스트와 음성과 같은 정해진 규칙이 없는 비정형 데이터 검색이 가능하며 전문 검색(full text)과 구조 검색 모두를 지원한다. 또한 기본적으로 검색엔진이지만 MongoDB나 Hbase와 같은 대용량 스토리지로도 활용이 가능하다는 특징을 가진다.
Elasticsearch의 동작 과정은 첫째, 로그, 시스템, 메트릭, 웹 애플리케이션 등 다양한 소스로부터 원시 데이터가 Elasticsearch로 흘러들어간다. 둘째, Elasticsearch는 검색 시간을 단축할 수 있는 Inverted Index(역 인덱스) 구조를 만들어 수집된 정보를 색인(indexing) 한다. 여기서 일반적인 색인의 목적은 ‘문서의 위치’에 대한 index를 만들어서 빠르게 그 문서에 접근하고자 하는 것인데, 역색인은 반대로 ‘문서 내의 문자와 같은 내용물’의 매핑 정보를 색인 해놓는 것이다. 따라서 역색인 구조를 통해서 특정 단어를 찾을 때, 문서 전체에서 찾는 것이 아닌 단어가 포함된 특정 문서의 위치를 알아내어 빠르게 결과를 찾아낼 수 있다. 마지막으로 사용자의 질의를 JSON 형식으로 입력받아 색인기에서 저장한 역색인 구조에서 일치하는 문서를 찾아 JSON 형식으로 결과로 반환한다.
Elasticsearch는 현재 웹 문서 검색, 소셜 데이터 분석, 쇼핑몰 검색 등에 활용되고 있으며, 빅데이터 분석/처리 및 MSA 환경의 로그 모니터링 등에도 활용되고 있다. Elasticsearch는 여러 분야에서 사용되고 있다. 그 예시로 삼성 SDS는 Elasticsearch 기반으로 Chatbot Birty를 구축하고 있고, 무료 여행 가이드 어플리케이션 Triple에서는 Elastic Cloud를 메인 플랫폼으로 사용하는 등 다양한 분야에서 Elasticsearch를 사용하고 있다.













GPT-Neo
 GPT-Neo는 비영리 오픈소스 연구 단체인 Eleuther AI에서 개발되었으며, GPT-3의 구조를 활용하여 학습한 거대 언어 모델로서, 학습 및 테스트에 필요한 코드들이 오픈소스로 공개되어 있을 뿐 아니라 학습에 사용된 대규모 데이터 셋인 Pile과 pre-trained model도 함께 공개되어 있다. GPT-Neo는 자동적인 언어 모델로서 훈련된다. 핵심적인 기능은 텍스트 문자열을 찍고 다음 토큰의 문자열을 예측하는 것이다. Eleuther가 그린 GPT-Neo의 다이어그램은 다음 사진처럼 나타낼 수 있다.
 
![image](https://user-images.githubusercontent.com/98390300/205444908-8d01f419-48e5-44a1-a143-c086face34bc.png)

 
정확한 문장을 만들 수 있는 언어 모델인 GPT-3은 OpenAI가 개발했지만 오픈소스 오픈 액세스가 아니고 마이크로소프트와 독점 라이선스 계약을 체결하지 않았기 때문에 자유롭게 사용할 수 없다. 이 때문에 GPT-3의 오픈소스 버전인 GPT-Neo가 개발되었다. 따라서 이 둘은 매우 유사한 아키텍처를 가진 언어 모델이다. 즉, 이 둘의 구조는 비슷하다고 보면 될 것 같다. 
GPT-Neo는 대규모 병렬 학습을 위한 라이브러리인 mesh-tensorflow 기반으로 만들어졌으며, 1.3B개, 2.7B개의 파라미터를 가지는 모델의 pre-trained model이 공개되어 있다. (*메쉬 텐서 플로우란 광범위한 분산 텐서 계산 클래스를 지정할 수 있는 분산 딥러닝용 언어이다) 
GPT-3는 미세 조정을 하지 않고 Few-Shot Learning을 통해 많이 증가한 데이터량을 해결하려고 했다. (Few-Shot Learning이란 적용하고 싶은 task에 대한 데이터 10~100개 정도를 사용하여 모델에게 알려준 뒤, 원하는 결과를 얻어내는 것을 말한다) 또한, 학습을 진행하면서 동시에 Few-show Learning으로 사칙 연산, 오타 검색, 번역 등의 다양한 패턴 인지 능력을 학습시켰다. 다음 그림의 각 sequence는 전체 학습을 시킬 동안 내부적으로 반복 과정을 통해 학습한다.

![image](https://user-images.githubusercontent.com/98390300/205444902-83480278-1d4b-405a-a280-ac9871a27c45.png)


GPT-3의 학습 방법은 다음 사진과 같다.

![image](https://user-images.githubusercontent.com/98390300/205444899-271d9656-6d69-4091-a0be-d725bd0cba40.png)


네이버 오픈 API
첫 번째로는 네이버의 쇼핑 인사이트와 검색어 트렌드를 활용하기 위해 네이버 데이터 랩 API를 사용한다. POST 방식의 메서드, JSON 응답 형식으로 데이터를 반환해 주면 정보를 받아와 애플리케이션에 적용한다.

두 번째로는 네이버의 뉴스, 백과사전, 블로그, 쇼핑, 지식in 등 실용성 있는 데이터를 GET 메서드, JSON, XML 응답 형식으로 반환받는다.

두 가지 방법 모두 비로그인 방식으로 네이버 로그인의 인증을 통한 접근 토큰을 가지고 있을 필요 없으며 HTTP 헤더에 클라이언트 시크릿 값만 전송하여 사용할 수 있는 API이다.

Google Custom Search API
애플리케이션에서 이미지를 보내면, 쿼리에 있는 제품을 업체의 제품 세트에 있는 이미지와 비교한 다음 시각적 및 의미론적으로 유사한 결과의 순위 목록을 GET 메서드를 활용하여 JSON 데이터 형식으로 반환해 준다. 이것을 적용하면 되지만 검색량이 많아질수록 요금을 지불해야 하기 때문에 요금을 지불하여 API를 사용해야 한다.


4. 라이센스

[스크린샷 2022-12-03 23 07 08](https://user-images.githubusercontent.com/98390300/205444878-f80dc5af-6a19-4579-b4b5-ecdef100de08.png)
	
Elastic-search	Elastic License	*상업적 이용시 계약 체결 혹은 솔라로 교체

4.1 상업적 이용시 변경 가능 사항
엘라스틱 서치
- 실시간 색인 가능, 계층 구조의 다양한 속성 검색/연관검색 가능함
- 검색속도 빠름 색인하는데 초 단위로 걸림
- 사이즈가 큰 장문 데이터 검색 시 속도 저하
- 주요 활용 영역 : 상품 검색, 이상징후 감지&모니터링
솔라
- 안정화 단계의 검색
- 사이즈가 큰 장문 데이터 검색에 용이
- 검색속도 느림
- 색인하는데 수 십분 걸림
- 계층 구조의 속성 검색 힘듦
- 주요 활용 영역 : 문서/원문 검색

5. DFD
 
![image](https://user-images.githubusercontent.com/98390300/205444858-29859e41-1dbd-448c-8c3c-4c7ddddec4c5.png)

사용자가 제품이 포함된 동영상을 챗봇에게 보내면 챗봇은 openCV에 영상을 전송해서 물체를 개별 프레임으로 만들어 Google Cloud API에 전송한다. 이 때 Google Cloud API에서 특정 제품으로 인식이 되면 제품에 관련된 키워드를 보여주고, 없으면 사용자에게 추가 검색 버튼을 누를 수 있게 설계하였다. 이후 추가 검색 버튼을 누르면 openCV에서 추출한 이미지가 챗봇을 통해 OCR로 간다. OCR에서는 사진 속 물체 정보(글자, 단어 등)를 추출하고, keyBERT에 전송한다. keyBERT는 챗봇에게 생성된 키워드를 전달 한다. 챗봇은 받은 키워드를 바탕으로 2개의 API, 1개의 연관검색어 크롤러를 통해 겹치는 키워드가 많은 경우 우선적으로 나열하여 사용자에게 전달한다. 이후 사용자는 원하는 키워드 항목을 선택하면 키워드에 관련된 제품 정보를 사용자에게 반환한다.
 만약 사용자가 제시된 키워드 이외의 결과를 원하는 경우, 챗봇을 통해 입력을 받고 문장의 형식일 경우 gpt-neo에게 전송하여 답을 받아온다. 사용자의 의도를 파악하지 못한 경우 입력했던 문장을 khaiii 로 받아온 뒤 keyBERT로 전송하여 키워드를 뽑아 ElasticSearch를 통하여 사용자가 원하는 답을 반환한다.

6. 예상 GUI
	Spring Boot, JS, Thymeleaf, BootStrap5 사용
    6.1.컴퓨터
6.1.2 초기화면

![image](https://user-images.githubusercontent.com/98390300/205444855-e072d812-a26b-4498-87c7-b7ded2e231d9.png)

① 서비스 이름
② 챗봇 채팅창
③ 비디오 선택창
④ 챗봇에게 채팅 보내기
6.1.3	진행화면

![image](https://user-images.githubusercontent.com/98390300/205444848-97e4469c-6ec7-44bd-9dec-28c2b6ced83f.png)


6.2	모바일
6.2.1초기화면

![image](https://user-images.githubusercontent.com/98390300/205444845-968ffa37-0df1-4147-a488-2d7639273b16.png)


6.2.2진행화면

![image](https://user-images.githubusercontent.com/98390300/205444839-c2b93e93-6ba1-49ae-8115-465e496c56f0.png)


7. 참고문헌
OpenCV
https://darkpgmr.tistory.com/131

https://bkshin.tistory.com/entry/OpenCV-26-%EC%9D%B4%EB%AF%B8%EC%A7%80%EC%9D%98-%ED%8A%B9%EC%A7%95%EA%B3%BC-%ED%82%A4-%ED%8F%AC%EC%9D%B8%ED%8A%B8

https://blog.naver.com/PostView.nhn?blogId=junghs1040&logNo=222242950987&parentCategoryNo=&categoryNo=26&viewDate=&isShowPopularPosts=true&from=search

khaiii 

https://tech.kakao.com/2018/12/13/khaiii/

GPT-Neo

https://blog.naver.com/jaeyoon_95/222311993385

https://smilegate.ai/2021/04/08/gpt-neo/

https://medium.com/data-science-rush/open-source-gpt-3-is-out-9ad81b7b6a30

Elasticsearch

https://jaemunbro.medium.com/elastic-search-%EA%B8%B0%EC%B4%88-%EC%8A%A4%ED%84%B0%EB%94%94-ff01870094f0

https://www.elastic.co/kr/what-is/elasticsearch

keyBert

https://insightcampus.co.kr/2021/07/08/keybert%EB%A1%9C-%EA%B4%80%EB%A0%A8-%ED%82%A4%EC%9B%8C%EB%93%9C-%EC%B6%94%EC%B6%9C%ED%95%98%EA%B8%B0/

https://heeya-stupidbutstudying.tistory.com/entry/DL-keyword-extraction-with-KeyBERT-%EA%B0%9C%EC%9A%94%EC%99%80-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-1

Tesseract-OCR

https://velog.io/@mare-solis/BERT-keyBert%EB%A1%9C-%ED%82%A4%EC%9B%8C%EB%93%9C-%EC%B6%94%EC%B6%9C%ED%95%98%EA%B8%B0

https://velog.io/@agugu95/Tesseract-OCR

김남규, 김동언, 김성우, 권순각, 「광학 문자 인식을 통한 단어 정리 방법」, 한국멀티미더어학회, p.943-p.949

BERT as a service: 

https://hanxiao.io/2019/01/02/Serving-Google-BERT-in-Production-using-Tensorflow-and-ZeroMQ/

https://blog.naver.com/dmswldla91/222167659381

BERT

https://happy-obok.tistory.com/23

ZeroMQ

https://ko.wikipedia.org/wiki/ZeroMQ

Transformer

https://blog.naver.com/winddori2002/222008003445
