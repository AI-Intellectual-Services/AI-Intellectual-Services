# 시스템 설계서     
Elasticsearch

Elasticsearch는 Apache Lucene(아파치 루씬) 기반의 분산 검색 엔진이다. Elasticsearch를 통해 Java에서 개발한 정보 검색용 라이브러리인 루씬 라이브러리를 단독으로 사용할 수 있으며, 방대한 양의 데이터를 신속하게 저장, 검색, 분석을 수행할 수 있다. Elasticsearch는 아파치 라이선스 조항에 의거하여 오픈소스로 출시되어 있다. 공식 클라이언트들은 자바(Java), C#닷넷(.NET), PHP, 파이썬(Python), 그루비(Groovy) 등 수많은 언어로 이용이 가능하다.

Elasticsearch는 여러 문서에서 특정 문자열을 검색하는 분산처리를 통해 실시간성으로 빠른 검색이 가능하다. 특히 기존의 데이터로 처리하기 힘든 대량의 텍스트와 음성과 같은 정해진 규칙이 없는 비정형 데이터 검색이 가능하며 전문 검색(full text) 검색과 구조 검색 모두를 지원한다. 또한 기본적으로는 검색엔진이지만 MongoDB나 Hbase와 같은 대용량 스토리지로도 활용이 가능하다는 특징을 가진다.

Elasticsearch의 동작 과정은 첫째, 로그, 시스템, 메트릭, 웹 애플리케이션 등 다양한 소스로부터 원시 데이터가 Elasticsearch로 흘러들어간다. 둘째, Elasticsearch는 이를 검색 시간을 단축할 수 있는 Inverted Index(역 인덱스) 구조를 만들어 수집된 정보를 색인(indexing) 한다. 여기서 역색인이란 일반적인 색인의 목적은 ‘문서의 위치’에 대한 index를 만들어서 빠르게 그 문서에 접근하고자 하는 것인데, 역색인은 반대로 ‘문서 내의 문자와 같은 내용물’의 매핑 정보를 색인 해놓는 것이다. 따라서 역색인 구조를 통해서 특정 단어를 찾을 때 문서 전체에서 찾는 것이 아닌, 단어가 포함된 특정 문서의 위치를 알아내어 빠르게 결과를 찾아낼 수 있다. 마지막으로 사용자의 질의를 JSON형식으로 입력받아 색인기에서 저장한 역색인 구조에서 일치하는 문서를 찾아 JSON 형식으로 결과로 반환한다.

Elasticsearch는 현재 웹 문서 검색, 소셜 데이터 분석, 쇼핑몰 검색 등에 활용되고 있으며, 빅데이터 분석/처리 및 MSA 환경의 로그 모니터링 등에도 활용되고 있다. 삼성 SDS는 Elasticsearch 기반으로 Chatbot Birty를 구축, 무료 여행 가이드 어플리케이션 Triple에서는 Elastic Cloud를 메인 플랫폼으로 사용하는 등 여러 분야에서 Elasticsearch를 사용하고 있다.

keyBERT

자연어 처리를 위해 구글에서 고안한 트랜스포머 기반의 머신러닝 기법인 BERT를 적용한 오픈 소스 파이썬 모듈이다. keyBERT는 원본 문서를 가장 잘 나타내는 중요한 용어 또는 구문을 찾아내는 키워드 추출 작업을 한다.

keyBERT의 원리는 BERT를 이용해 문서 레벨 (document-level)에서의 주제 (representation)를 파악하도록 하고, N-gram을 위해 단어(자연어)를 기계가 이해할 수 있는 숫자의 나열인 벡터로 바꾸는 임베딩 작업을 한다. 여기서 N-gram 이란 Bag of Words, TF-IDF(Term Frequency - Inverse Document Frequency) 와 같이 사용되는 횟수 기반의 벡터 표현 방식을 사용하는 언어 모델이다. 이후 N-gram 모델을 사용해 문서의 각 문장을 단어로 쪼개준다. 코사인 유사도를 계산하여 어떤 N-gram 단어 또는 구가 문서와 가장 유사한지 찾아낸다. 그리고 가장 유사한 단어들은 문서를 가장 잘 설명할 수 있는 키워드로 분류된다.


또한, keyBERT는 중복된 의미의 키워드가 너무 많이 추출되는 경우에 대비해 결과 키워드에 다양성을 도입하는 MSS, MMR 두 가지 방법을 포함한다. MSS는 candidate-document 간 거리는 최소로 하면서 candidate-candidate 간 거리는 최대로 함으로써 의미적으로 풍부한 키워드 set을 얻는다. 문서와 가장 유사한 words/phrases 2개를 사용해 이 두 개의 top_n 단어에서 모든 top_n combination을 취해 코사인 유사도를 구한다. 여기서 유사도가 가장 낮은 조합을 추출하는 방법론이다. MMR은 텍스트 요약 작업에서 중복성을 최소화하고 결과의 다양성을 극대화하기 위해 노력한다. 문서와 가장 유사한 키워드를 선택하는 것으로 시작한다. 그런 다음 문서와 비슷하면서도 이미 선택한 키워드와 비슷하지 않은 새 후보를 반복적으로 선택한다.

이처럼 keyBERT는 비교적 단순함에도 불구하고 효율적이기 때문에 키워드 추출을 수행하는 경우 유용하게 사용된다.

Khaiii

khaiii는 “Kakao Hangul Analyzer III”의 줄임말로 카카오에서 개발한 세 번째 형태소 분석기이다. khaiii는 세종 코퍼스를 이용하여 CNN(Convolutional Neural Network, 합성곱 신경망) 기술을 적용해 학습한 형태소 분석기이다. 디코더를 C++로 구현하여 GPU 없이도 비교적 빠르게 동작하며, Python 바인딩을 제공하고 있어서 편리하게 사용할 수 있다.

규칙 기반으로 동작하여 사람이 직접 규칙을 입력해야 했던 기존의 분석기인 dha1, dha2 와는 다르게 khaiii는 데이터 기반으로 동작하기 때문에 기계학습 알고리즘(딥러닝)을 사용한다. 기계학습을 위해서 신경망 알고리즘들 중에서 CNN(Convolutional Neural Network)을 사용했다. 입력하는 경우 각각의 음절이 분류 대상이다.

형태소 분석 결과를 다시 형태소 각각의 음절 별로 나우어 IOB1 방식으로 나타내면 다음 사진과 같고, 나누어진 각각의 음절들은 그 다음 사진과 같이 정렬된다.

![Image.png](https://res.craft.do/user/full/8ce8fb2c-58d3-136f-9cce-c7e23418d06c/doc/D78E9B0C-210A-4811-A026-DD5CC75719F3/EDCBCF14-39AD-486F-928B-5686E496B269_2/c2zj0USD9L3RKccSFxSpU6GE3nys3i9iJRpIm55x6o8z/Image.png)

![Image.png](https://res.craft.do/user/full/8ce8fb2c-58d3-136f-9cce-c7e23418d06c/doc/D78E9B0C-210A-4811-A026-DD5CC75719F3/A2EF7B97-B1EE-45DC-BA27-CCC5F2988A6F_2/ErgHgy41eIf126nDfkk9Zfuy27RTMOBY4IFdWcFimb4z/Image.png)

khaiii의 성능에 대해 얘기해보면, 정확도에서는 파라미터 win의 경우, 3 또는 4에서 가장 좋은 성능을 보였고 그 이상에서는 성능이 떨어졌다. 파라미터 emb의 경우, 150까지는 성능도 같이 높아지다가 그 이상에서는 별 차이가 없다. 다음 사진은 정확도에 관한 사진이다.
GPT-Neo
비영리 오픈소스 연구단체인 Eleuther AI에서 발표한 GPT-Neo는 GPT -3의 구조를 활용하여 학습한 거대 언어 모델로서, 학습 및 테스트에 필요한 코드들이 오픈소스로 공개되어 있을 뿐 아니라 학습에 사용된 대규모 데이터셋인 Pile과 pre-trained model도 함께 공개되어 있다. GPT-Neo 는 자동적인 언어 모델로서 훈련된다. 핵심적인 기능은 텍스트 문자열을 찍고 다음 토큰의 문자열을 예측하는 것이다.
정확한 문장을 만들 수 있는 언어 모델인 GPT-3은 OpenAI가 개발했지만 오픈소스 오픈 액세스가 아니고 마이크로소프트와 독점 라이선스 계약을 체결하지 않았기 때문에 자유롭게 사용할 수 없다. 이런 상황에 대해 오픈소스와 같은 GPT-3을 만든 것이 GPT-Neo이다. 이 둘은 매우 유사한 아키텍처를 가진 언어 모델이다. 즉, 이 둘의 구조는 비슷하다고 보면 될 것 같다.
GPT-Neo는 대규모 병렬학습을 위한 라이브러리인 mesh-tesorflow 기반으로 만들어 졌으며, 1.3B개, 2.7B개의 파라미터를 가지는 모델의 pre-trained model이 공개되어 있다. (*메쉬 텐서 플로우란 광범위한 분산 텐서 계산 클래스를 지정할 수 있는 분산 딥러닝용 언어이다)
GPT-3는 미세 조정을 하지 않고 Few-Shot Learning을 통해 많이 증가한 데이터량을 해결하려고 했다. (Few-Shot Learning이란 적용하고 싶은 task에 대한 데이터 10~100개 정도를 사용하여 모델에게 알려준 뒤, 원하는 결과를 얻어내는 것을 말한다)
GPT-3는 학습을 진행하면서 동시에 few-show learning으로 사칙 연산, 오타 검색, 번역 등의 다양한 패턴 인지 능력을 학습시켰다. 다음 그림의 각 sequence는 전체 학습을 시킬 동안 내부적으로 반복 과정을 통해 학습한다.

